{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To identify what a web page is about using NLTK in Python, you would generally perform the following\n",
    "# steps:\n",
    "# 1. Fetch the Web Page Content: Use libraries like requests or BeautifulSoup to scrape the text\n",
    "# content of the web page.\n",
    "# 2. Text Preprocessing: Clean the text by removing HTML tags, stop words, and punctuation.\n",
    "# 3. Topic Identification: Use NLP techniques such as word frequency analysis, named entity\n",
    "# recognition (NER), or topic modeling to identify the main topics or themes of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ne_chunk, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch the Web Page Content\n",
    "def fetch_webpage_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract text from all paragraph tags\n",
    "\n",
    "        page_text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "        return page_text\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch the webpage. Status code {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    return None\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the Text\n",
    "def preprocess_text(text):\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "# Convert to lowercase\n",
    "tokens = [word.lower() for word in tokens]\n",
    "# Remove punctuation and non-alphabetic characters\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "# Remove stopwords\n",
    "words = [word for word in words if word not in stop_words]\n",
    "return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Frequency Analysis to Identify Topics\n",
    "def identify_topics(words, num_topics=10):\n",
    "fdist = FreqDist(words)\n",
    "common_words = fdist.most_common(num_topics)\n",
    "return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Named Entity Recognition (NER)\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "ner_tree = ne_chunk(pos_tags, binary=False)\n",
    "return ner_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Program\n",
    "if __name__ == '__main__':\n",
    "# URL of the web page to analyze\n",
    "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "\n",
    "# Step 1: Fetch Web Page Content\n",
    "page_content = fetch_webpage_content(url)\n",
    "if page_content:\n",
    "print(\"Web Page Content Fetched Successfully!\")\n",
    "\n",
    "# Step 2: Preprocess the Text\n",
    "processed_text = preprocess_text(page_content)\n",
    "print(f\"Processed Text Sample: {processed_text[:20]}\")\n",
    "\n",
    "# Step 3: Identify Topics\n",
    "topics = identify_topics(processed_text)\n",
    "print(\"\\nMost Common Topics Based on Word Frequency:\")\n",
    "for word, freq in topics:\n",
    "print(f\"{word}: {freq}\")\n",
    "\n",
    "# Step 4: Named Entity Recognition (NER)\n",
    "print(\"\\nNamed Entities in the Web Page:\")\n",
    "ner_result = named_entity_recognition(page_content)\n",
    "\n",
    "ner_result.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
